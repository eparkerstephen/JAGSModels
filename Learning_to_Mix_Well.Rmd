Mixture Modeling with JAGS
==========================

```{r, echo=FALSE}
rm(list=ls())
require(knitr)
opts_chunk$set(comment=NA, cache=FALSE)
pFormat <- function(x) {formatC(x, format="f", digits=2)}
```

This page is about estimating a mixture models using [JAGS](http://martynplummer.wordpress.com/jags/) via [R](http://cran.us.r-project.org/).

## Two Component Mixture of Normals

The first mixture model examined considers a distribution comprised of two normal distributions.  First things first, we create the data, pooling two distinct normally-distributed variables -- ```x1```, with a mean of 3 and standard deviation of 2; and ```x2```, with a mean of 10 and standard deviation of 2.  The last puts the data into the list format that JAGS requires.  

```{r, echo = TRUE, message = FALSE, warning = FALSE}
require(coda)
require(rjags)
require(reshape)
x1 <- rnorm(25, 3, 2)
x2 <- rnorm(75, 10, 2)
x <- c(x1,x2)
data <- list(x=x,n=length(x))
```


The JAGS code for a two-component mixture of normals is:

    model{
      p ~ dbeta(1,1)
      mu1 ~ dnorm(10, .01)
      mu2 ~ dnorm(50, .01)
      tau <- pow(sigma, -2)
      sigma ~ dunif(0,100)
      for (i in 1:n)
          {
          z[i] ~ dbern(p)
          mu[i] <- z[i]*mu1 + (1-z[i])*mu2
          x[i] ~ dnorm(mu[i], tau)
          }
    }

The parameters we are estimating are the means of the two component distributions, ```mu1``` and ```mu2```.  The model results also provide values ```z[i]``` for each observation's probability of belonging to one of the two component distributions.  

We pass initial values to JAGS in order to initialize the MCMC simulation.  Note that each ```x[i]``` gets its own mean, but all observations share the variance, which in this case is set to equal 1.

```{r, echo = TRUE, message = FALSE, warning = FALSE}
init <- list(mu=array(rnorm(1*data$n)),mu1=rnorm(1), mu2=rnorm(1),sigma=1)
```


```{r, echo = FALSE, warning = FALSE, message = FALSE}
# read in JAGS model using the textConnection
modelstring="
  model {
    for (i in 1:n)
        {
        z[i] ~ dbern(p)
        mu[i] <- z[i]*mu1 + (1-z[i])*mu2
        x[i] ~ dnorm(mu[i], tau)
        }
    p ~ dbeta(1,1)
    mu1 ~ dnorm(60, .001)
    mu2 ~ dnorm(90, .001)
    tau <- pow(sigma, -2)
    sigma ~ dunif(0,100)
  }
"
```

We pass the model to JAGS using the ```textConnection()``` function.  The text is entered into ```R``` as the object ```modelstring```.  I prefer to run the MCMC simulation using the ```coda.samples()``` function in the **coda** package.  Another popular package for doing so is **R2Jags**.

Of particular interest is whether the model can recover the (known) means of the two component distributions (```mu1``` = 3, and ```mu2``` = 10).  Figure 1 plots the simulated values for these parameters.  Despite using highly uninformative priors, the means are easily recovered.

```{r, echo = TRUE, results = 'hide', warning = FALSE, message = FALSE}
model <- jags.model(textConnection(modelstring), data=data, n.adapt=1e5)
# Run samples using coda package sampler
out <- coda.samples(model=model, init=init, variable.names=c("mu1", "mu2", "z"), n.iter=1e5, thin=20)
# Extract elements from list: our estimates of the two means
mu1 <- unlist(out[,"mu1",])
mu2 <- unlist(out[,"mu2",])
```


**Figure 1.** Density estimation for the simulated values of ```mu1``` and ```mu2```
```{r, echo = FALSE, fig.width = 7, fig.height = 4, fig.align='center', message=FALSE, warning=FALSE, dpi = 144, cache=FALSE}
require(ggplot2)
mu1 <- data.frame(value = mu1, parameter = rep("mu1", length(mu1)))
mu2 <- data.frame(value = mu2, parameter = rep("mu2", length(mu2)))
df <- rbind(mu1, mu2)
p <- ggplot(df, aes(value, colour=parameter, fill=parameter, colour=parameter))
p <- p + geom_histogram(binwidth=.1, position='stack') +
  scale_x_continuous("value") +
  scale_y_continuous("count") +
  scale_fill_brewer(palette="Accent") +
  scale_colour_brewer(palette="Accent")
print(p)
```

A second point of interest is related to the estimates of belonging to a particular group.  In the model, the ```z[i]``` parameter is distributed Bernoulli.  Thus, at each iteration, every observation is assigned a 0 or 1, indicating the observation's mean is either the sampled value for ```mu1``` or ```mu2```.  Figure 2 presents this assignment variable for the ```z[1]``` parameter at each iteration (left panel) along with the density estimate (right panel).

**Figure 2.** Traceplot and density estimate for the ```z[1]``` parameter
```{r, echo=TRUE, fig.align='center', fig.width=7, fig.height=6}
plot(out[,"z[1]",])
```



The average for ```z[i]``` across the entire simulation represents the probability that an observation belongs to one or the other components.  Observations with a value that is close to one mean have sharper assignment probabilities -- that is, an average value that is close to either 0 or 1.


Figure 2 plots these average probabilities, where the numeric labels are the ```x[i]``` values.  Values that are close to the means are highly likely to have been drawn from one distribution or the other.  The model has difficulty assigning probabilities to those few observations that fall at roughly the midpoint between 3 and 10.


**Figure 2.** Each observation's probability of belonging to the ```mu1``` distribution

```{r, echo = FALSE, fig.width=7, fig.height = 14, fig.align='center', message = FALSE, warning = FALSE, dpi=100}
z <- out[[1]][,grep("z", colnames(out[[1]]))]
zbar <- colMeans(z)
zbar <- melt(zbar)
probs <- data.frame(zbar = zbar, obs = rownames(zbar))
p <- ggplot(probs, aes(value, reorder(obs, value)))
p <- p + geom_point(size=0) + 
  theme(axis.text=element_text(size=4, colour='black')) +
  scale_x_continuous("average of sampled z[i]") +
  scale_y_discrete("observation") +
  geom_text(aes(x=value, y=obs, label = pFormat(x)), size = 2)
print(p)
```

### What if there are three distributions?

In the simulation, we knew ```x``` was a mixture of two random normal variables.  What happens when we attempt to use the two-component normal model with data that is more accurately characterized by a three-component mixture?  Let's find out by simulating some new data and re-running. 

```{r, echo = TRUE, message = FALSE, warning = FALSE}
x1 <- rnorm(20, 3, 2)
x2 <- rnorm(60, 10, 2)
x3 <- rnorm(20, 20, 4)

x <- c(x1,x2,x3)
data <- list(x=x,n=length(x))
```


```{r, echo = TRUE, results = 'hide', warning = FALSE, message = FALSE}
model <- jags.model(textConnection(modelstring), data=data, n.adapt=1e5)
# Run samples using coda package sampler
out <- coda.samples(model=model, init=init, variable.names=c("mu1", "mu2", "z"), n.iter=1e5, thin=20)
# Extract elements from list: our estimates of the two means
mu1 <- unlist(out[,"mu1",])
mu2 <- unlist(out[,"mu2",])
```


As is clear in Figure 3, the separation is no longer evident.  The reason is illustrated in Figure 4, which shows that low data points are pushed into a single component.  The solution to this problem is to allow for the three components explicitly in the statistical model.

**Figure 3.** Density estimation for the simulated values of ```mu1``` and ```mu2```
```{r, echo = FALSE, fig.width = 7, fig.height = 4, fig.align='center', message=FALSE, warning=FALSE, dpi = 144, cache=FALSE}
require(ggplot2)
mu1 <- data.frame(value = mu1, parameter = rep("mu1", length(mu1)))
mu2 <- data.frame(value = mu2, parameter = rep("mu2", length(mu2)))
df <- rbind(mu1, mu2)
p <- ggplot(df, aes(value, colour=parameter, fill=parameter, colour=parameter))
p <- p + geom_histogram(binwidth=.1, position='stack') +
  scale_x_continuous("value") +
  scale_y_continuous("count") +
  scale_fill_brewer(palette="Accent") +
  scale_colour_brewer(palette="Accent")
print(p)
```


**Figure 4.** Each observation's probability of belonging to the ```mu1``` distribution

```{r, echo = FALSE, fig.width=7, fig.height = 14, fig.align='center', message = FALSE, warning = FALSE, dpi=100}
z <- out[[1]][,grep("z", colnames(out[[1]]))]
zbar <- colMeans(z)
zbar <- melt(zbar)
probs <- data.frame(zbar = zbar, obs = rownames(zbar))
p <- ggplot(probs, aes(value, reorder(obs, value)))
p <- p + geom_point(size=0) + 
  theme(axis.text=element_text(size=4, colour='black')) +
  scale_x_continuous("average of sampled z[i]") +
  scale_y_discrete("observation") +
  geom_text(aes(x=value, y=obs, label = pFormat(x)), size = 2)
print(p)
```

